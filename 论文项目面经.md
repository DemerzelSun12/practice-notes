[TOC]
# 三篇论文

## Fast GPU-based Subgraph Search Using Parallel Vertex Matching

子图匹配的过程是通过查找与**输入查询图同构的数据图**的所有子图来执行的。在现实生活中的数据图上执行此操作通常需要大量计算，因为需要检查大量图顶点。 现有的子图匹配方案均采用简单的方案，将查询图的顶点一一匹配。 该策略无法利用图的结构并行性，并且可能会导致 GPU 上的大量内存访问。

这项工作提出了 GENEVA，一种新的**基于 GPU 的子图匹配方案，可以同时匹配多个查询顶点**。 与之前的工作不同，GENEVA 在单个 GPU 内核中执行子图匹配，消除了处理中间结果所需的许多内存访问操作。GENEVA还提供了增强的存储格式，以减少图数据的内存占用并提高处理效率。我们通过将我们的方法应用于 NVIDIA 2080Ti GPU 上的八个现实图形数据集来评估我们的方法。

- 提出了一种新颖的并行顶点匹配方法，支持同时处理多个查询顶点，以减少 GPU 全局内存访问操作；
- 它提出了一种增强的存储格式，以提高存储效率并减少顶点搜索时间（第4节）；
- 它引入了增强的匹配顺序生成算法来生成适当的顶点匹配顺序以支持高效的子图匹配。

### 图存储格式

仅将当前计算所需的部分数据存储在GPU内存中。 因此，在不影响计算性能的情况下找到图数据的紧凑表示非常重要。 为此，GENEVA 扩展了分区压缩稀疏行（PCSR），该格式**将具有相同标签的边分组为边标签分区，然后以压缩稀疏行（CSR）稀疏矩阵存储格式存储**。通过这样做，**只有与匹配边具有相同边标签的分区才需要从CPU内存移动到GPU内存**。 PCSR 对 CSR 格式做了一些小的修改。顶点 ID (VID) 可用于查找顶点的行偏移，因为 VID 在经典 CSR 中存储为连续元素。将边分组为分区后，不再保证顶点的 VID 在分区中是连续的。

为了减少 PCSR 哈希函数所需的未使用内存空间量，记录边缘标签分区 (ELP) 的连续 VID 范围。 连续的VID范围被记录在区间索引结构中，该结构存储了第一个VID和该范围内的连续VID的数量。
由于ELP可能包含很多小区间，直接将每个要存储的区间映射到区间索引中会导致很多区间索引。 这并不理想，因为拥有大量间隔索引意味着我们需要多次内存访问才能找到 VID 的间隔。我们的设计通过仔细地将原始 VID 映射到新 VID 来避免这一陷阱，从而允许在每个边缘标签分区中生成更多连续的新 VID，从而减少间隔数量。本方法的数据存储格式与霍夫曼编码有着相同的思路——希望在处理最大的ELP（即包含最大数量的VID的分区）时减少内存访问的次数。

转化步骤

1. 步骤1：找到最大的ELP。首先选择包含最多顶点的分区（第 3 行）。我们的直觉是，分区包含的顶点越多，访问它的概率就越高。 因此，减少该分区的间隔数量有助于减少平均内存访问延迟。
2. 重命名 VID。 在第二步中，我们尝试通过重命名 VID 来增加连续 VID 间隔。 为此，**对于每个顶点，我们找出包含该顶点的所有 ELP。** 接下来，我们**将具有相同共享集的 VID 合并到子分区** subPart中。 我们将这种分组策略应用于步骤1中找到的最大ELP。**对于属于同一子分区的顶点，我们可以以任意顺序为它们分配唯一的、连续的新VID，只要这些新VID是连续的并且遵循最大的 最后处理的子分区的 VID。**
3. 处理子分区。 为了确定要处理的 ELP 子分区的顺序，我们从大多数 ELP 共享的子分区开始。这意味着对于大多数情况，我们将首先处理 subpart0，因为它具有最大的共享集。 我们选择的下一个子分区将是**其共享集和最后选择的子分区的共享集之间具有最多公共 ELP 数量的子分区**。 我们选择这种策略有以下两个原因。 
   1. 首先，两个子分区的共享集中的共同元素越多，拥有这两个子分区的 ELP 就越多。 
   2. 其次，因为我们确保两个连续处理的子分区之间的VID是连续的，所以我们可以将两个子分区的VID合并以形成一个连续的区间来存储在单个区间索引中。使用这种策略，我们可以使用单个区间索引来记录图4d⃝2中子部分0和1的新VID。 对于每个选定的子分区，我们将新的连续 VID 分配给选定子分区中的旧 VID（第 9-10、15-17 行）。
4. 在停止之前重复此操作。我们从 ELP 中删除已处理的顶点，并依次对每个 ELP 重复步骤 1 到 4。此过程将停止，直到所有 VID 均已重命名并记录在间隔索引中。

## 匹配模式

采用两步方法来执行图搜索。 在扩展阶段，GENEVA 匹配查询和数据图之间的图顶点和边标签以生成嵌入（即匹配的子图）。在消除阶段，我们删除与指定的非树查询边不匹配的嵌入。

### 扩展阶段

对于给定的查询图和GENEVA存储格式的数据图，我们在多个扩展阶段迭代地执行子图匹配。 每个扩展阶段处理图 1 中描述的匹配模式之一。以前的工作 [10]、[11] 使用传统的单顶点匹配（SV-match）方案。 该方案需要对 GPU 内存进行大量的加载和存储操作，因为它需要在匹配查询顶点后写入中间结果，并在匹配下一个查询顶点之前读取相同的中间结果。 GENEVA 的设计就是为了避免这个陷阱。
在 GPU 上执行子图搜索的一个关键障碍是 GPU warp（基本 GPU 调度单元 - 参见第 2.2 节）生成的嵌入数量不同，我们必须仔细计算用于存储新生成的嵌入的内存位置 对于每个扭曲以避免写入冲突。 先前的工作通过生成两次嵌入（以便使用第一代来计算存储位置）[7]或需要访问所有嵌入两次来计算存储位置[10]来解决这个问题。 GENEVA 采用了不同的方法，利用扩展内核内的 CUDAatomicAdd 原语来直接计算跨 GPU 包装的写入地址，从而无需访问或生成两次嵌入。 该原子指令确保在任何给定时间只有一个包装可以更新相同的变量，从而避免了确定跨包装的嵌入的存储位置的竞争条件。 虽然存在与atomicAdd相关的开销，但我们发现由于图结构固有的不规则性，这在子图搜索中并不是一个严重的问题。

总体工作流程。 对于每个嵌入（第 3 行），我们首先根据要处理的匹配模式从嵌入中获取源 VID（第 4 行）。接下来，我们在加载的 ELP 的 GENEVA 区间-PCSR 中搜索源 VID 并提取它们的邻居（第 5 行）。 然后，我们删除无效的邻居，这些邻居要么具有错误的顶点标签，要么不满足限制（第 6-7 行）。 最后，我们使用不同的算法为不同的匹配模式生成新的嵌入（第 8-15 行）。



### 消除阶段

在消除阶段，GENEVA 删除与指定的非树查询边不匹配的难以辨认的嵌入。在我们的方法中，我们匹配尽可能多的非树边，以消除早期阶段的无效嵌入。消除阶段主要流程： 对于每个嵌入 emb（第 2 行），我们首先检查消除阶段的所有查询边是否可以在 emb 中匹配（第 4 行）。 如果是这样（第 5 行），我们将 emb 写入共享内存 tmp（第 6 行），如果全局内存已满，则将 tmp 写入全局内存（第 7-9 行）。

1. 为了尽快消除无效的嵌入，我们需要首先匹配核心结构中的圆，如图3中的{u0，u1，u2}。因此，在匹配圆时，我们只需要匹配最多两个顶点，因为圆中的顶点恰好有两个邻居。 匹配核心结构后，我们可以使用任何适当的匹配模式来匹配树。
2. 基于我们的并行顶点匹配方法，首先生成对嵌入中 VID 的限制（第 1 行），目的是避免生成自同构嵌入。 然后，我们通过迭代地从 q 中删除 1 度顶点来生成 q 的核心结构，并使用删除的顶点构造 q 的树（第 2 行）。
3. 为了匹配核心结构，我们首先找到核心结构中的最小圆（第3行），然后选择圆中最高优先级匹配模式的顶点（第4行）。第一个扩展阶段从边标签分区中获取源顶点（图3⃝1）。生成初始阶段后，我们迭代地在圆中查找符合匹配的顶点，并选择具有最高优先级的顶点（第 7-9 行）。 最后，我们使用圆中的非树边构建消除阶段（第10行）。
4. 分解最小圆后，我们迭代地为核心结构中的其余顶点和边构建扩展和消除阶段（第 11 行）。 
   1. 首先，我们找到所有非树边并按边标签对它们进行分组（第 12-13 行）。 
   2. 然后对于每个组，我们构建一个淘汰阶段（第 14-15 行）。 如果没有找到非树边，我们在核心结构中搜索可以形成匹配模式0-4之一的不匹配顶点，并选择具有最高优先级的顶点（第16-18行）。
匹配核心结构后，我们可以使用图1中任何适当的匹配模式来匹配不在核心结构中的剩余顶点，因为没有留下非树边。 在每次迭代中，我们找到可以形成匹配的顶点，并选择具有最高优先级的顶点（第 19-21 行）。

## 实验部分

数据集

|Type of sizes|	Graph Name	| V |E |LV |LE|
|:--:|:--:|:--:|:--:|:--:|:--:|
|Tiny	|Enron|	36K	|183K|	3	|3|
|Tiny|	FirstMM	|56K	|126K|	3	|3|
|Small|	DD|	0.3M|	0.8M	|5|	5|
|Small|	Gowalla	|0.2M|	0.9M	|5	|5|
|Medium|	Patents	|3.7M|	16M	|7|	7|
|Medium|	Reddit	|4.6M	|5.5M|	7	|7|
|Large|	Orkut|	3M	|117M|	12|	12|
|Large	|sinaweibo|	58M	|261M|	12	|12|



## EVRM: Elastic Virtual Resource Management Framework for Cloud Virtual Instances

设计了一个自适应弹性虚拟资源管理（EVRM）框架。 在EVRM的核心设计中，构建了弹性资源管理模型，并提出了一种基于深度强化学习的方法，旨在提高资源利用率和虚拟实例运行性能。可以显着提高资源利用率和虚拟实例运行性能，分别针对虚拟机和Docker环境做了不同优化。

核心目的，解决三个难题：
1. 由于虚拟实例的运行性能通常无法获得具体的定量结果，因此大多数研究**仅在分配**虚拟资源**或收集系统信息时考虑资源利用率**。 因此，管理弹性虚拟资源管理的第一个挑战是：如何构建优化虚拟实例运行性能的弹性虚拟资源管理模型？
2. 目前广泛使用的是**基于反馈的资源分配，仅针对当前系统状态获得最佳虚拟资源分配。**然而，由于服务器上虚拟实例的动态工作负载，这些分配解决方案**对于未来的系统状态可能不是最佳的**。而且普通的强化学习（RL）方法**在资源分配时只能考虑有限的动作空间**，**忽略了连续的动作空间**，导致资源分配性能有限。 因此，第二个挑战是：如何获得资源分配解决方案来优化服务器上虚拟实例的动态工作负载的适应？

针对CH1，我们分析了影响虚拟实例资源利用率和运行性能的关键因素。 此外，资源管理方法需要与云环境交互，在交互过程中，根据当前状态选择每个资源分配并执行以转换到下一个状态。 因此，资源管理问题遵循马尔可夫决策过程（MDP）。 我们最终构建了弹性虚拟资源管理问题的MDP模型。

针对CH2，由于无法完全理解复杂的依赖关系和不断变化的工作负载来估计资源需求，我们应用深度强化学习（DRL）来学习分配多个虚拟资源的优化策略并在虚拟实例运行期间调整其决策在服务器上。学习到的模型可以最大化潜在奖励，即资源利用率和虚拟实例运行性能的整体优化。 具体来说，我们提出了一种基于深度确定性策略梯度的资源分配（DDPG-RA）方法，以协调服务器上虚拟实例的**CPU、内存和带宽资源**。 DDPG-RA采用**深度确定性策略梯度（DDPG）**在训练过程中自动探索优化策略，学习多个虚拟资源分配、资源利用率和虚拟实例运行性能之间的微妙关系。 

本文的贡献包括：
- 构建了一种新颖的弹性虚拟资源管理模型，指出了影响虚拟实例资源利用率和运行性能的关键因素。 据我们所知，我们是第一个研究协调 CPU、内存和带宽的人。
- 提出了一种自适应资源管理算法，称为DDPG-RA，该算法首先采用DDPG模型和提出的动作细化算法来动态定量地计算具有最大潜在奖励的目标资源分配。
- 开发了自适应资源管理框架EVRM，应用DDPG-RA 所学知识来适应工作负载的动态变化，以提高资源利用率和虚拟实例运行性能为目标。

## 一、系统主要结构

该架构是一个事件循环系统，由部署在主机中的三个主要模块组成：监视器、计算器和执行器模块。
- 监控器收集每个虚拟实例的多种资源信息，包括CPU、内存、网络。 我们在研究过程中探索了几种获取资源信息的方法。
- 计算器模块定期从监视器模块检索所有虚拟实例的资源信息，并为执行器模块做出全局决策，以使用我们提出的资源管理算法自动为多个虚拟实例调度多个资源。
- 执行器为每个虚拟实例提供目标资源。

### 监控器

1. 采用libvirt工具包来实现中间VM监视器，通过分析CPU和内存信息
分别是 virDomain.info() 和 virDomain.MemoryStats()。 前者在第四个返回参数中显示了所使用的 CPU 时间（以纳秒 (ns) 为单位）。 同样，我们获取每个VM的可用内存大小。 另外，我们还可以获取通信包信息
```bash
/proc/net/dev
```
2. 通过 docker stats 命令来分析 Docker 容器的资源信息，包括 CPU、内存和网络，该命令通过 Docker 守护进程记录 cgroup 文件系统下的 CPU 和内存参数的值。

### 计算器

计算模块定期从监控模块获取所有虚拟实例的资源信息，并为执行器模块做出全局决策，自动在多个实例之间调度资源。 为了保持监视器和计算器模块之间的同步，EVRM 定期监视和调度资源。 计算器模块使用我们提出的资源调度算法DDPG-RA，根据每个周期的当前工作负载计算每个实例的目标资源分配。 DDPG-RA是计算器模块的核心。

### 执行器

从计算器模块获取这些目标资源配置值后，EVRM 通过执行器模块执行资源分配。
1. KVM执行器：KVM管理程序有一个名为kvm.ko的内核模块，它管理虚拟CPU和内存。该接口由 `libvirt` 提供，而内存分配函数 `virDomainSetMemoryFlags()` 和 CPU 分配函数 `virDomainSetVcpus()` 可以分别动态更改分配给每个 VM 的内存和 vCPU 目标量。 **我们还可以使用virsh管理工具来控制内存和CPU分配。** **我们通过Linux TC工具控制每个vNIC的带宽，即限制最大上传（下载）带宽。**
2. Docker Executor：我们的EVRM应用原始的docker更新配置工具，通过参数`--cpus`和`-m`来控制内存和CPU分配。 最后，与VM类似，我们采用Linux TC工具为容器分配带宽资源。




## 二、弹性虚拟资源管理问题的马尔科夫决策模型

我们框架的目标是**通过充分利用多种资源来提高虚拟实例的运行性能**，每个虚拟实例应该释放其未充分利用的资源并获得其过度利用的资源。

这里我们为每个资源定义两个阈值。
- 对于内存利用率，定义一个内存利用率的下限阈值。当低于此阈值时，内存利用率未得到充分利用，应释放其内存资源。同时，也会定义为内存利用率的上限阈值。 当虚拟实例使用的内存多于 Ψmhigh 时，虚拟实例会因内存利用率过载而遭受性能下降，并且应该增加内存大小。
- 我们定义了两组 CPU 和带宽利用率阈值。

此外，有些虚拟实例资源利用率过高，有些虚拟实例负载较轻，这是不合理的。 因此，更好的资源分配选择是在虚拟实例之间获得均衡的资源利用率。 也会定义为 t 时刻三种资源利用率的方差之和。

在MDP过程中，**根据当前状态st选择执行具有最大潜在奖励的资源分配动作at，执行该动作后获得即时奖励rt，并进入新的状态st+1。**
为了有效决策分配内存、CPU和带宽，状态集包含每个虚拟实例vi的资源信息，包括三个配置资源{mi(t),ci(t),bi(t)}的数量 以及三种资源{umi,uci,ubi}对应的利用率。

## 实验数据集

Java 基准套件 DaCapo

选择了几个基准应用程序来评估性能开销，包括**内存密集型**（即 h2）、**处理器密集型**（即 jython、pmd、avrora、sunflow、fop、xalan、lusearch、lusearch-fix、batik）、**磁盘 - 密集**（即 eclipse、luindex）基准测试。 

Httpload 是一个基于 Linux 的 Web 服务器测试工具。 给定固定数量的并发数 p 和 fetch f（说明每次测试中获取的数据量），可以根据测试的完成时间来估计网络性能。 在所有评估中，我们设置 p = 50 和 f = 1000。


## Minimizing Service Latency through Image-based Microservice Caching and Randomized Request Routing in Mobile Edge Computing

在边缘计算环境中提出了一种近似算法，实现一种兼顾服务缓存效率和任务路由的算法，在KubeEdge环境下完成，是一个基于k8s的边缘计算解决方案。

kubeedge的云侧组件可以deployment的方式部署在云侧，包括自定义的controller和云边通信组件；边侧组件可以认为是包括一个阉割的kubelet在内的用于边缘侧应用管理和设备管理的组件。不恰当地说，部署完成后，整个云和边都在同一个k8s集群内。

### 对pod控制方法

采用 k8s 原生的pod资源获取办法，资源考虑维度，CPU和内存占用

```bash
kubectl top pod
kubectl top pod |grep -E “$podname1|$podname2|…”
```

动态调整 pod 资源办法

Kubernetes 1.27 中的 alpha 功能发布。其中一项能够自动调整 Pod 的 CPU 和内存限制的大小，只需修补正在运行的 Pod 定义即可更改它们，而无需重新启动它。

InPlacePodVerticalScaling 必须启用功能门。
```bash
FEATURE_GATES=InPlacePodVerticalScaling=true ./hack/local-up-cluster.sh
```

一旦本地集群启动并运行，Kubernetes 用户就可以通过 kubectl 调度 pod 的资源并调整 pod 的大小。

修改cpu配置

```bash
kubectl patch pod podname --patch '{"spec":{"containers":[{"name":"podname", "resources":{"limits":{"cpu":"3"}}}]}}'
```

修改内存配置
```bash
kubectl patch pod name --patch '{ "spec" :{ "containers" :[{ "name" : "podname" , "podname" :{ "limits" :{ "memory" : "2Gi" }}}]} }'
```


Kubernetes 就地更新底层 c-group 分配，从而使 pod 资源定义可变。这在垂直扩展 pod 的情况下特别有用，例如使用 Kubernetes 内置的Vertical Pod Autoscaler (VPA)，它允许应用程序在同一 pod 内向上/向下扩展资源（而不是通过更多pod 进/出进行扩展）与传统的水平 Pod 缩放一样）。

### 近似算法

在问题的求解过程中，对于算法性能的评估，一般都出于三个方面：

- 解的优越性，即是否能达到最优解
- 算法的效率，即复杂度（能否在多项式时间内完成）
- 算法适用的范围，即是否适用于所有情况，还是只适合特殊情形

一般的算法在这三个方面往往不能同时表现得很好，但是我们可以退而求其次，选择其中得两个方面去尽可能地满足，当我们选择满足后两者，即对解的优越性放宽要求时，设计出的算法被称为近似算法。


# 项目

## KeepE5Alive

GitHub Actions是一个CI/CD解决方案，它可以轻松地设置周期性任务以自动化软件工作流。

通过多线程调用 Microsoft Graph API，每个线程都会自动选择 API 进行调用，然后将结果记录下来，

处理令牌方法是使用一个基于 Express 框架的简单的 Web 服务器，它用于处理 Microsoft 登录的回调请求并获取刷新令牌，使用 Puppeteer 进行自动化测试的脚本，主要功能是模拟用户登录 Microsoft 平台，并进行授权操作。

github action

GitHub Actions 是 GitHub 提供的一项持续集成和持续部署（CI/CD）服务，它可以帮助开发者自动化软件开发过程中的各种任务，比如代码测试、构建、部署等。GitHub Actions 的工作模式可以概括如下：

1. 触发事件（Event Triggering）：

GitHub Actions 可以通过多种事件触发执行，包括代码的推送（Push）、Pull Request 的创建或更新、Issue 的创建或更新、定时任务等。当这些事件发生时，GitHub 会触发相应的 Actions 执行。
2. 选择工作流程（Workflow Selection）：

在 GitHub 仓库中，可以定义多个工作流程（Workflow），每个工作流程可以包含一个或多个任务（Job）。通过配置 YAML 文件（通常命名为 .github/workflows/main.yml），可以指定工作流程的触发条件和执行步骤。
3. 执行任务（Job Execution）：

每个工作流程可以包含一个或多个任务（Job），每个任务定义了一系列需要执行的步骤。这些步骤可以是运行命令行命令、调用外部脚本或程序、上传下载文件等。GitHub Actions 提供了丰富的执行环境和预装软件支持，比如 Ubuntu、Windows、macOS 等操作系统，以及 Node.js、Python、Java、Docker 等常用软件环境。
4. 并发执行（Parallel Execution）：

工作流程中的多个任务可以并发执行，可以指定依赖关系和执行顺序。GitHub Actions 会自动优化任务的并发执行，以缩短整体执行时间。
5. 状态检查（Status Checks）：

在任务执行过程中，GitHub Actions 会监控任务的执行状态，并及时反馈给用户。用户可以在 GitHub 页面上查看任务的执行日志、状态和执行时间等信息。
6. 持续集成/持续部署（CI/CD）：

GitHub Actions 可以用于实现持续集成和持续部署，例如在代码推送时自动运行测试、构建 Docker 镜像、部署应用程序等。用户可以通过配置工作流程和任务来实现自己的 CI/CD 流程，提高软件开发的效率和质量。


## 硕士研究点

在离线DNN任务混合部署策略以及再部署

### CPU、GPU编排

设计了面向应用的 QoS 语义和 CPU 和 GPU 编排协议。
1. LS（Latency Sensitive）应用于典型的微服务负载，将其与其它的延迟敏感型负载隔离保障其性能。
2. LSR（Latency Sensitive Reserved）类似于 Kubernetes 的 Guaranteed，在 LS 的基础上增加了应用要求预留绑核的语义。
3. LSE（Latency Sensitive Exclusive）则常见于中间件等对 CPU 特别敏感的应用，除了满足其类似于 LSR 要求绑核的语义外，还确保其所被分配的 CPU 不与任何其它负载共享。

### 再部署

调度器中支持的**负载感知调度能够在调度时选择负载较低的节点运行新的Pod**，但**随着时间、集群环境变化以及工作负载面对的流量、请求的变化时，节点的利用率会动态的发生变化，**集群内节点间原本负载均衡的情况被打破，甚至有可能出现极端负载不均衡的情况，影响到工作负载运行时质量。再部署功能负责感知集群内节点负载的变化，自动的优化超过负载水位安全阈值的节点，防止出现极端负载不均衡的情况。目前已经实现了基于多资源检测的负责感知方法，实时获取物理节点真实资源利用率，后续会逐渐完善再部署的节点选择与任务暂停恢复方法，实现工作负载的可用性保障和全局流控等安全性策略，保障集群的高可用性。



