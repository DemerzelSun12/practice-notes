[TOC]

# Kubernetes 面试题

## 一、K8s简述

### 1.1 简述什么是 Kubernetes？

Kubernetes 是一个全新的基于容器技术的分布式系统支撑平台。是 Google 开源的容器集群管理系统（谷歌内部:Borg）。在 Docker 技术的基础上，为容器化的应用提供部署运行、资源调度、服务发现和动态伸缩等一系列完整功能，提高了大规模容器集群管理的便捷性。并且具有完备的集群管理能力，多层次的安全防护和准入机制、多租户应用支撑能力、透明的服务注册和发现机制、內建智能负载均衡器、强大的故障发现和自我修复能力、服务滚动升级和在线扩容能力、可扩展的资源自动调度机制以及多粒度的资源配额管理能力。

### 1.2 简述 Kubernetes 和 Docker 的关系？

Docker 提供容器的生命周期管理和 Docker 镜像构建运行时容器。它的主要优点是将将软件/应用程序运行所需的设置和依赖项打包到一个容器中，从而实现了可移植性等优点。

Kubernetes 用于关联和编排在多个主机上运行的容器。

### 1.3 简述 Kubernetes 中什么是 Minikube、Kubectl、Kubelet？

- Minikube 是一种可以在本地轻松运行一个单节点 Kubernetes 群集的工具。
- Kubectl 是一个命令行工具，可以使用该工具控制 Kubernetes 集群管理器，如检查群集资源，创建、删除和更新组件，查看应用程序。
- Kubelet 是一个代理服务，它在每个节点上运行，并使从服务器与主服务器通信。

### 1.4 简述 Kubernetes 常见的部署方式？

答：常见的 Kubernetes 部署方式有：

- kubeadm：也是推荐的一种部署方式；
- 二进制：
- minikube：在本地轻松运行一个单节点 Kubernetes 群集的工具。



### 1.5 简述 Kubernetes 如何实现集群管理？

在集群管理方面，Kubernetes 将集群中的机器划分为一个 Master 节点和一群工作节点 Node。其中，在 Master 节点运行着集群管理相关的一组进程 kube- apiserver、kube-controller-manager 和 kube-scheduler，这些进程实现了整个集群的资源管理、Pod 调度、弹性伸缩、安全控制、系统监控和纠错等管理能力，并且都是全自动完成的。

### 1.6 简述 Kubernetes 的优势、适应场景及其特点？

Kubernetes 作为一个完备的分布式系统支撑平台，其主要优势：
- 容器编排
- 轻量级
- 开源
- 弹性伸缩
- 负载均衡

Kubernetes 常见场景：

- 快速部署应用
- 快速扩展应用
- 无缝对接新的应用功能
- 节省资源，优化硬件资源的使用

Kubernetes 相关特点：

- 可移植: 支持公有云、私有云、混合云、多重云（multi-cloud）。
- 可扩展: 模块化,、插件化、可挂载、可组合。
- 自动化: 自动部署、自动重启、自动复制、自动伸缩/扩展。

## 二、组件相关

### 2.1 简述 Kubernetes 相关基础概念？

- master：k8s 集群的管理节点，负责管理集群，提供集群的资源数据访问入口。拥有 Etcd 存储服务（可选），运行 Api Server 进程，Controller Manager 服务进程及 Scheduler 服务进程。
- node（worker）：Node（worker）是 Kubernetes 集群架构中运行 Pod 的服务节点，是 Kubernetes 集群操作的单元，用来承载被分配 Pod 的运行，是 Pod 运行的宿主机。运行 docker eninge 服务，守护进程kunelet 及负载均衡器kube-proxy。
- pod：运行于 Node 节点上，若干相关容器的组合。Pod 内包含的容器运行在同一宿主机上，使用相同的网络命名空间、IP 地址和端口，能够通过 localhost 进行通信。Pod 是Kurbernetes 进行创建、调度和管理的最小单位，它提供了比容器更高层次的抽象，使得部署和管理更加灵活。一个 Pod 可以包含一个容器或者多个相关容器。
- label：Kubernetes 中的 Label 实质是一系列的 Key/Value 键值对，其中 key 与value 可自定义。Label 可以附加到各种资源对象上，如 Node、Pod、Service、RC 等。一个资源对象可以定义任意数量的 Label，同一个Label 也可以被添加到任意数量的资源对象上去。Kubernetes 通过 Label Selector（标签选择器）查询和筛选资源对象。
- Replication Controller：Replication Controller 用来管理 Pod 的副本，保证集群中存在指定数量的 Pod 副本。集群中副本的数量大于指定数量，则会停止指定数量之外的多余容器数量。反之，则会启动少于指定数量个数的容器，保证数量不变。Replication Controller 是实现弹性伸缩、动态扩容和滚动升级的核心。
- **Deployment**：Deployment 在内部使用了 RS 来实现目的，Deployment 相当于 RC 的一次升级，其最大的特色为可以随时获知当前 Pod 的部署进度。
- **HPA（Horizontal Pod Autoscaler）**：Pod 的横向自动扩容，也是 Kubernetes 的一种资源，通过追踪分析RC 控制的所有 Pod 目标的负载变化情况，来确定是否需要针对性的调整 Pod 副本数量。
- Service：Service 定义了 Pod 的逻辑集合和访问该集合的策略，是真实服务的抽象。Service 提供了一个统一的服务访问入口以及服务代理和发现机制，关联多个相同Label 的 Pod，用户不需要了解后台 Pod 是如何运行。
- Volume：Volume 是 Pod 中能够被多个容器访问的共享目录，Kubernetes 中的Volume 是定义在 Pod 上，可以被一个或多个 Pod 中的容器挂载到某个目录下。
- Namespace：Namespace 用于实现多租户的资源隔离，可将集群内部的资源对象分配到不同的Namespace 中，形成逻辑上的不同项目、小组或用户组，便于不同的 Namespace 在共享使用整个集群的资源的同时还能被分别管理。

### 2.2 简述 Kubernetes 集群相关组件？

Kubernetes Master 控制组件，调度管理整个系统（集群），包含如下组件:

1. Kubernetes **API Server**：作为 Kubernetes 系统的入口，其封装了核心对象的增删改查操作，以RESTful API 接口方式提供给外部客户和内部组件调用，**集群内各个功能模块之间数据交互和通信的中心枢纽**
2. Kubernetes **Scheduler**：为新建立的 Pod 进行节点(node)选择(即分配机器)，负责集群的资源调度。
3. Kubernetes **Controller**：负责执行各种控制器，目前已经提供了很多控制器来保证Kubernetes 的正常运行。
   1. Replication Controller：管理维护 Replication Controller，关联 Replication Controller 和 Pod，保证Replication Controller 定义的副本数量与实际运行Pod 数量一致。
   2. Node Controller：管理维护 Node，定期检查 Node 的健康状态，标识出(失效| 未失效)的 Node 节点。
   3. Namespace Controller：管理维护 Namespace，定期清理无效的 Namespace，包括 Namesapce 下的 API 对象，比如 Pod、Service 等。
   4. Service Controller：管理维护 Service，提供负载以及服务代理
   5. EndPoints Controller：管理维护 Endpoints，关联 Service 和 Pod，创建Endpoints 为 Service 的后端，当 Pod 发生变化时，实时更新 Endpoints。
   6. Service Account Controller：管理维护 Service Account，为每个 Namespace 创建默认的Service Account，同时为 Service Account 创建 Service Account Secret。
   7. **Deployment Controller**：管理维护 Deployment，关联 Deployment 和Replication Controller，保证运行指定数量的 Pod。当 Deployment 更新时， 控制实现 Replication Controller 和 Pod 的更新。
   8. **Job Controller**：管理维护 Job，为 Jod 创建一次性任务 Pod，保证完成 Job 指定完成的任务数目。


Kubernetes Node 组件，node节点才是k8s集群中中工作负载节点，每个node节点都会被Master节点分配工作负载。

1. kubelet: 会监视已分配给节点的pod，负责pod的生命周期管理，同时与Master密切协作，维护和管理该Node上面的所有容器，实现集群管理的基本功能。即Node节点通过kubelet与master组件交互，可以理解为kubelet是Master在每个Node节点上面的agent。本质上，它负责使Pod的运行状态与期望的状态一致。
2. kube-proxy: 是实现service的通信与负载均衡机制的重要组件，将到service的请求转发到后端的pod上。
3. docker-engine(containerd): 是负责容器的创建和管理工作。

### 2.3 简述 Kubernetes RC 的机制？

Replication Controller 用来**管理 Pod 的副本**，保证集群中存在指定数量的 Pod 副本。当定义了 RC 并提交至Kubernetes 集群中之后，Master 节点上的 Controller Manager 组件获悉，并同时巡检系统中当前存活的目标Pod，并确保目标 Pod 实例的数量刚好等于此 RC 的期望值，若存在过多的 Pod 副本在运行，系统会停止一些Pod，反之则自动创建一些 Pod。

### 2.4 简述 Kubernetes Replica Set 和 Replication Controller 之间有什么区别？

Replica Set 和 Replication Controller 类似，都是确保在任何给定时间运行指定数量的 Pod 副本。不同之处在于 **RS 使用基于集合的选择器**，而 **Replication Controller 使用基于权限的选择器。**

### 2.5 简述 kube-proxy 作用？

kube-proxy 运行在所有节点上，它**监听 apiserver 中 service 和 endpoint 的变化情况**，创建路由规则以**提供服务 IP 和负载均衡功能**。简单理解此进程是 Service 的透明代理兼负载均衡器，其核心功能是将到某个 Service 的访问请求转发到后端的多个 Pod 实例上。

### 2.6 简述 kube-proxy iptables 原理？

Kubernetes 从 1.2 版本开始，将 iptables 作为 kube-proxy 的默认模式。iptables 模式下的 kube-proxy 不再起到 Proxy 的作用，其核心功能：**通过 API Server 的Watch 接口实时跟踪 Service 与Endpoint 的变更信息，并更新对应的iptables 规则**，Client 的请求流量则通过 iptables 的 NAT 机制“直接路由”到目标Pod。

### 2.7 简述 kube-proxy ipvs 原理？

IPVS 在 Kubernetes1.11 中升级为 GA 稳定版。IPVS 则专门用于高性能负载均衡， 并使用更高效的数据结构（Hash 表），允许几乎无限的规模扩张，因此被 kube-proxy 采纳为最新模式。

在 IPVS 模式下，使用 iptables 的扩展 ipset，而不是直接调用 iptables 来生成规则链。iptables 规则链是一个线性的数据结构，ipset 则引入了带索引的数据结构，因此当规则很多时，也可以很高效地查找和匹配。

可以将 ipset 简单理解为一个 IP（段）的集合，这个集合的内容可以是 IP 地址、IP 网段、端口等，iptables 可以直接添加规则对这个“可变的集合”进行操作，这样做的好处在于可以大大减少 iptables 规则的数量，从而减少性能损耗。



### 2.8 简述 kube-proxy ipvs 和 iptables 的异同？

iptables 与 IPVS 都是基于 Netfilter 实现的，但因为定位不同，二者有着本质的差别：iptables 是为防火墙而设计的；IPVS 则专门用于高性能负载均衡，并使用更高效的数据结构（Hash 表），允许几乎无限的规模扩张。

与 iptables 相比，IPVS 拥有以下明显优势：
1. 为大型集群提供了更好的可扩展性和性能；
2. 支持比 iptables 更复杂的复制均衡算法（最小负载、最少连接、加权等）；
3. 支持服务器健康检查和连接重试等功能；
4. 可以动态修改 ipset 的集合，即使 iptables 的规则正在使用这个集合。

## 三、pod相关

### 3.1 简述 Kubernetes 中什么是静态 Pod？

静态 pod 是由 kubelet 进行管理的**仅存在于特定 Node 的 Pod 上，他们不能通过 API Server 进行管理，无法与 ReplicationController、Deployment 或者DaemonSet 进行关联，**并且 kubelet 无法对他们进行健康检查。静态 Pod 总是由kubelet 进行创建，并且总是在 kubelet 所在的 Node 上运行。

### 3.2 简述 Kubernetes 中 Pod 可能位于的状态？

1. Pending：API Server 已经创建该 Pod，且 Pod 内还有一个或多个容器的镜像没有创建，包括正在下载镜像的过程。
2. Running：Pod 内所有容器均已创建，且至少有一个容器处于运行状态、正在启动状态或正在重启状态。
3. Succeeded：Pod 内所有容器均成功执行退出，且不会重启。
4. Failed：Pod 内所有容器均已退出，但至少有一个容器退出为失败状态。
5. Unknown：由于某种原因无法获取该 Pod 状态，可能由于网络通信不畅导致。

### 3.3 如何创建pod或者创建deployment流程？(必问)


#### 一、如何创建pod流程：

1. 客户端提交创建请求，可以通过API Server的Restful API，也可以使用如何命令行工具。支持的数据类型包括JSON和YAML。
2. API Server处理用户请求，存储Pod数据到etcd。
3. 调度器通过API Server查看未绑定的Pod。尝试为Pod分配主机。
4. 过滤主机 (调度预选)：调度器用一组规则过滤掉不符合要求的主机。比如Pod指定了所需要的资源量，那么可用资源比Pod需要的资源量少的主机会被过滤掉。
5. 主机打分(调度优选)：对第一步筛选出的符合要求的主机进行打分，在主机打分阶段，调度器会考虑一些整体优化策略，比如把容一个Replication Controller的副本分布到不同的主机上，使用最低负载的主机等。
6. 选择主机：选择打分最高的主机，进行binding操作，结果存储到etcd中。
7. kubelet根据调度结果执行Pod创建操作：绑定成功后，scheduler会调用APIServer的API在etcd中创建一个boundpod对象，描述在一个工作节点上绑定运行的所有pod信息。运行在每个工作节点上的kubelet也会定期与etcd同步boundpod信息，一旦发现应该在该工作节点上运行的boundpod对象没有更新，则调用Docker API创建并启动pod内的容器。


#### 二、如何创建deployment流程？（tips：这个要回答出创建rs的过程）

1. 准备好一个包含应用程序的Deployment的yml文件，然后通过kubectl客户端工具发送给ApiServer。
2. ApiServer接收到客户端的请求并将资源内容存储到数据库(etcd)中。
3. Controller组件(包括scheduler、replication、endpoint)监控资源变化并作出反应。
4. ReplicaSet检查数据库变化，创建期望数量的pod实例。
5. Scheduler再次检查数据库变化，发现尚未被分配到具体执行节点(node)的Pod，然后根据一组相关规则将pod分配到可以运行它们的节点上，并更新数据库，记录pod分配情况。
6. Kubelete监控数据库变化，管理后续pod的生命周期，发现被分配到它所在的节点上运行的那些pod。如果找到新pod，则会在该节点上运行这个新pod。
7. kuber proxy运行在集群各个主机上，管理网络通信，如服务发现、负载均衡。例如当有数据发送到主机时，将其路由到正确的pod或容器。对于从主机上发出的数据，它可以基于请求地址发现远程服务器，并将数据正确路由，在某些情况下会使用轮训调度算法(Round-robin)将请求发送到集群中的多个实例。
8. kubectl提交一个请求，来创建RC，此时Controller Manager通过API server里的接口监听到这个RC事件，分析之后，发现当前集群中还没有它对应的Pod实例，于是根据RC里的Pod模板定义Pod对象；接下来，此事件被Scheduler发现，它立即执行一个复杂的调度流程，为这个新Pod选定一个落户的Node，这个过程可称为绑定；随后模板Node上运行的Kubelet进程通过API Server监测到这个“新生的”Pod并按照它的定义，启动Pod并负责后期的管理；随后我们通过Kubectl提交一个映射到该Pod的Server的创建请求，Controller Manager会通过Label标签查询到相关联的Pod实例，然后生成Service的Endpoints信息；接下来，所有Node上运行的Proxy进程通过API Server查询并监听Service对象及其对应的Endpoints信息，建立一个负载均衡器来实现Service访问到后端Pod的流量转发功能；


### 3.4 简述 Kubernetes 中 Pod 的重启策略？

Pod 重启策略（RestartPolicy）应用于 Pod 内的所有容器，并且仅在 Pod 所处的 Node 上由 kubelet 进行判断和重启操作。当某个容器异常退出或者健康检查失败时，kubelet 将根据 RestartPolicy 的设置来进行相应操作。

Pod 的重启策略包括 Always、OnFailure 和 Never，默认值为 Always。
1. Always：当容器失效时，由 kubelet 自动重启该容器；
2. OnFailure：当容器终止运行且退出码不为 0 时，由 kubelet 自动重启该容器；
3. Never：不论容器运行状态如何，kubelet 都不会重启该容器。同时 Pod 的重启策略与控制方式关联，当前可用于管理 Pod 的控制器包括ReplicationController、Job、DaemonSet 及直接管理 kubelet 管理（静态 Pod）。不同控制器的重启策略限制如下：

1. RC 和 DaemonSet：必须设置为 Always，需要保证该容器持续运行；
2. Job：OnFailure 或 Never，确保容器执行完成后不再重启；
3. kubelet：在 Pod 失效时重启，不论将 RestartPolicy 设置为何值，也不会对 Pod 进行健康检查。

### 3.5 简述 Kubernetes 中 Pod 的健康检查方式？

对 Pod 的健康检查可以通过两类探针来检查：LivenessProbe 和ReadinessProbe。

1. LivenessProbe 探针：用于判断容器是否存活（running 状态），如果LivenessProbe 探针探测到容器不健康，则 kubelet 将杀掉该容器，并根据容器的重启策略做相应处理。若一个容器不包含 LivenessProbe 探针，kubelet 认为该容器的 LivenessProbe 探针返回值用于是“Success”。

2. ReadineeProbe 探针：用于判断容器是否启动完成（ready 状态）。如果ReadinessProbe 探针探测到失败，则 Pod 的状态将被修改。Endpoint Controller 将从 Service 的Endpoint 中删除包含该容器所在 Pod 的Eenpoint。

3. startupProbe 探针：启动检查机制，应用一些启动缓慢的业务，避免业务长时间启动而被上面两类探针 kill 掉。

### 3.6 简述 Kubernetes Pod 的常见调度方式？

Kubernetes 中，Pod 通常是容器的载体，主要有如下常见调度方式：

1. Deployment 或 RC：该调度策略主要功能就是自动部署一个容器应用的多份副本， 以及持续监控副本的数量，在集群内始终维持用户指定的副本数量。
2. NodeSelector：定向调度，当需要手动指定将 Pod 调度到特定 Node 上，可以通过 Node 的标签（Label）和 Pod 的nodeSelector 属性相匹配。
3. NodeAffinity 亲和性调度：亲和性调度机制极大的扩展了 Pod 的调度能力，目前有两种节点亲和力表达：
   1. required During Scheduling Ignored During Execution：硬规则，必须满足指定的规则，调度器才可以调度 Pod 至 Node 上（类似 nodeSelector，语法不同
   2. preferred During Scheduling Ignored During Execution：软规则，优先调度至满足的 Node 的节点，但不强求，多个优先级规则还可以设置权重值。
4. Taints 和 Tolerations（污点和容忍）；
5. Taint：使 Node 拒绝特定 Pod 运行；
6. Toleration：为 Pod 的属性，表示 Pod 能容忍（运行）标注了 Taint 的 Node。

### 3.7 简述 Kubernetes 初始化容器（init container）？

答：init container 的运行方式与应用容器不同，它们必须先于应用容器执行完成，当设置了多个 init container 时，将按顺序逐个运行，并且只有前一个 init container 运行成功后才能运行后一个 init container。当所有 init container 都成功运行后， Kubernetes 才会初始化 Pod 的各种信息，并开始创建和运行应用容器。

### 3.8 简述 Kubernetes 自动扩容机制？

答：Kubernetes 使用 **Horizontal Pod Autoscaler（HPA）**的控制器实现基于 CPU 使用率进行自动 Pod 扩缩容的功能。HPA 控制器周期性地监测目标 Pod 的资源性能指标，并与 HPA 资源对象中的扩缩容条件进行对比，在满足条件时对 Pod 副本数量进行调整。

HPA 原理

Kubernetes 中的某个 Metrics Server（Heapster 或自定义 Metrics Server）**持续采集所有 Pod 副本的指标数据。**HPA 控制器通过 Metrics Server 的 API（Heapster 的API 或聚合 API）获取这些数据，基于用户定义的扩缩容规则进行计算，得到目标Pod 副本数量。

当目标 Pod 副本数量与当前副本数量不同时，HPA 控制器就向 Pod 的副本控制器（Deployment、RC 或 ReplicaSet）发起 scale 操作，调整 Pod 的副本数量，完成扩缩容操作。



### 3.9 简述 Kubernetes Service 类型？

通过创建 Service，可以为一组具有相同功能的容器应用提供一个统一的入口地址， 并且将请求负载分发到后端的各个容器应用上。其主要类型有：

1. ClusterIP：虚拟的服务 IP 地址，该地址用于 Kubernetes 集群内部的 Pod 访问， 在 Node 上 kube-proxy 通过设置的 iptables 规则进行转发；
2. NodePort：使用宿主机的端口，使能够访问各 Node 的外部客户端通过 Node 的 IP 地址和端口号就能访问服务；
3. LoadBalancer：使用外接负载均衡器完成到服务的负载分发，需要在spec.status.loadBalancer 字段指定外部负载均衡器的 IP 地址，通常用于公有云。

### 3.10 简述 Kubernetes 外部如何访问集群内的服务？

对于 Kubernetes，集群外的客户端默认情况，无法通过 Pod 的 IP 地址或者Service 的虚拟 IP 地址:虚拟端口号进行访问。通常可以通过以下方式进行访问Kubernetes 集群内的服务：
1. 映射 Pod 到物理机：将 Pod 端口号映射到宿主机，即在 Pod 中采用hostPort 方式，以使客户端应用能够通过物理机访问容器应用。
2. 映射 Service 到物理机：将 Service 端口号映射到宿主机，即在 Service 中采用nodePort 方式，以使客户端应用能够通过物理机访问容器应用。
3. 映射 Sercie 到LoadBalancer：通过设置 LoadBalancer 映射到云服务商提供的LoadBalancer 地址。这种用法仅用于在公有云服务提供商的云平台上设置Service 的场景。

## 四、调度相关

### 4.1 简述 Kubernetes 各模块如何与 API Server 通信？

Kubernetes API Server 作为集群的核心，负责集群各功能模块之间的通信。集群内的各个功能模块通过 API Server 将信息存入 etcd，当需要获取和操作这些数据时，则通过 API Server 提供的 REST 接口（用 GET、LIST 或 WATCH 方法）来实现，从而实现各模块之间的信息交互。

- 如 kubelet 进程与 API Server 的交互：每个 Node 上的 kubelet 每隔一个时间周期， 就会调用一次 API Server 的REST 接口报告自身状态，API Server 在接收到这些信息后，会将节点状态信息更新到 etcd 中。
- 如 kube-controller-manager 进 程 与 API Server 的 交 互 ：kube-controller- manager 中的 Node Controller 模块通过 API Server 提供的 Watch 接口实时监控Node 的信息，并做相应处理。
- 如 kube-scheduler 进程与 API Server 的交互：Scheduler 通过 API Server 的Watch 接口监听到新建 Pod 副本的信息后，会检索所有符合该 Pod 要求的 Node 列表，开始执行 Pod 调度逻辑，在调度成功后将 Pod 绑定到目标节点上。

### 4.2 简述 Kubernetes Scheduler 作用及实现原理？

Kubernetes Scheduler 是 负 责 Pod 调 度 的 重 要 功 能 模 块 ，Kubernetes Scheduler 在整个系统中承担了“承上启下”的重要功能，**“承上”是指它负责接收Controller Manager 创建的新 Pod，为其调度至目标 Node**；**“启下”是指调度完成后，目标 Node 上的 kubelet 服务进程接管后继工作，负责 Pod 接下来生命周期**。Kubernetes Scheduler 的作用是将待调度的 Pod（API 新创建的 Pod、Controller Manager 为补足副本而创建的 Pod 等）按照特定的调度算法和调度策略绑定（Binding）到集群中某个合适的 Node 上，并将绑定信息写入 etcd 中。

在整个调度过程中涉及三个对象，分别是待**调度 Pod 列表、可用 Node 列表，以及调度算法和策略。**

Kubernetes Scheduler 通过调度算法调度为待调度 Pod 列表中的每个 Pod 从 Node 列表中选择一个最适合的Node 来实现 Pod 的调度。随后，目标节点上的 kubelet 通过 API Server 监听到 Kubernetes Scheduler 产生的Pod 绑定事件，然后获取对应的 Pod 清单，下载 Image 镜像并启动容器。

### 4.3 简述 Kubernetes Scheduler 使用哪两种算法将 Pod 绑定到 worker 节点？

Kubernetes Scheduler 根据如下两种调度算法将 Pod 绑定到最合适的工作节点：

1. 预选（Predicates）：输入是所有节点，输出是满足预选条件的节点。kube- scheduler 根据预选策略过滤掉不满足策略的 Nodes。如果某节点的资源不足或者不满足预选策略的条件则无法通过预选。如“Node 的label 必须与 Pod 的Selector 一致”。
2. 优选（Priorities）：输入是预选阶段筛选出的节点，优选会根据优先策略为通过预选的 Nodes 进行打分排名，选择得分最高的 Node。例如，资源越富裕、负载越小的 Node 可能具有越高的排名。

### 4.4 简述 Kubernetes kubelet 的作用？

在 Kubernetes 集群中，在每个 Node（又称 Worker）上都会启动一个 kubelet 服务进程。该进程用于处理 Master 下发到本节点的任务，管理 Pod 及 Pod 中的容器。

每个 kubelet 进程都会在 API Server 上注册节点自身的信息，定期向 Master 汇报节点资源的使用情况，并通过 cAdvisor 监控容器和节点资源。

### 4.5 简述 Kubernetes kubelet 监控Worker 节点资源是使用什么组件来实现的？

kubelet 使用 cAdvisor 对worker 节点资源进行监控。在 Kubernetes 系统中， cAdvisor 已被默认集成到 kubelet 组件内，当 kubelet 服务启动时，它会自动启动cAdvisor 服务，然后 cAdvisor 会实时采集所在节点的性能指标及在节点上运行的容器的性能指标。


### 4.6 简述 Kubernetes Worker 节点加入集群的过程？

通常需要对 Worker 节点进行扩容，从而将应用系统进行水平扩展。主要过程如下：

1. 在该 Node 上安装 Docker、kubelet 和 kube-proxy 服务；
2. 然后配置 kubelet 和 kubeproxy 的启动参数，将 Master URL 指定为当前Kubernetes 集群 Master 的地址，最后启动这些服务；
3. 通过 kubelet 默认的自动注册机制，新的 Worker 将会自动加入现有的Kubernetes 集群中；
4. Kubernetes Master 在接受了新 Worker 的注册之后，会自动将其纳入当前集群的调度范围。

### 4.7 简述 Kubernetes Pod 如何实现对节点的资源控制？

Kubernetes 集群里的节点提供的资源主要是**计算资源，计算资源是可计量的能被申请、分配和使用的基础资源**。当前 Kubernetes 集群中的计算资源主要包括 CPU、GPU 及Memory。CPU 与Memory 是被 Pod 使用的，因此在配置 Pod 时可以通过参数 CPU Request 及Memory Request 为其中的每个容器指定所需使用的 CPU 与Memory 量，Kubernetes 会根据 Request 的值去查找有足够资源的 Node 来调度此Pod。

通常，一个程序所使用的 CPU 与 Memory 是一个动态的量，确切地说，是一个范围，跟它的负载密切相关：负载增加时，CPU 和Memory 的使用量也会增加。

### 4.8 简述 Kubernetes Requests 和Limits 如何影响 Pod 的调度？

当一个 Pod 创建成功时，Kubernetes 调度器（Scheduler）会为该 Pod 选择一个节点来执行。对于每种计算资源（CPU 和Memory）而言，每个节点都有一个能用于运行 Pod 的最大容量值。调度器在调度时，首先要确保调度后该节点上所有 Pod 的 CPU 和内存的 Requests 总和，不超过该节点能提供给 Pod 使用的 CPU 和Memory 的最大容量值。

### 4.9 

## 五、其他组件


### 5.1 在异构架构下，比如有些主机是x86，有些主机是arm架构，k8s怎么保证异构架构下这些主机拉起不同的架构的镜像？

docker每一个镜像包含了一个文件，这个文件包含了有关于镜像信息，如层、大小和摘要。docker manifest命令还向用户提供附加信息，比如构建镜像的操作系统和体系结构。而manifest list是一个镜像清单列表，用于存放多个不同os/arch的镜像信息。我们主要用到manifest的目的，其实还是多用于存放不同的os/arch信息，也就是方便我们在不同的CPU架构（arm或者x86）或者操作系统中，通过一个镜像名称拉取对应架构或者操作系统的镜像，这个尤其是在K8S中，对于异构CPU的服务器中的镜像显得尤为有效。


### 5.2 Prometheus相关问题

pro的四种数据构：Counter，Gauge，Histogram，SummaryPrometheus
组成及架构 Prometheus 生态圈中包含了多个组件，其中许多组件是可选择的：
- 1.Prometheus Server: ------服务端 ---处理，储存数据
负责收集和存储时间，Prometheus 组成及架构序列数据（time series data），并且提供查询接口。
- 2.Jobs/Exporters: ------客户端 ---采集数据

客户端，用于暴露已有的第三方服务的 metrics 给 Prometheus。
监控并采集指标，对外暴露HTTP服务（/metrics）；目前已经有很多的软件原生就支持Prometheus，提供/metrics，可以直接使用；对于像操作系统已经不提供/metrics的应用，可以使用现有的exporters或者开发自己的exporters来提供/metrics服务；

- 3.Push Gateway: -----相当于代理---转发数据

针对push系统设计，Short-lived jobs定时将指标push到Pushgateway，再由Prometheus Server从Pushgateway上pull；主要用于短期的 jobs。由于这类 jobs 存在时间较短，可能在 Prometheus 来 pull 之前就消失了。为此，这次 jobs 可以直接向 Prometheus server 端推送它们的 metrics。这种方式主要用于服务层面的 metrics，对于机器层面的 metrices，需要使用 node exporter。

- 4.Alertmanager: ----告警方式---实现告警

报警组件，从 Prometheus server 端接收到 alerts 后，会进行去除重复数据，分组，并路由到对的接受方式，发出报警。常见的接收方式有：电子邮件，pagerduty，OpsGenie, webhook 等。

- 5.Web UI：

Prometheus内置一个简单的Web控制台，可以查询指标，查看配置信息或者Service Discovery等，实际工作中，查看指标或者创建仪表盘通常使用Grafana，Prometheus作为Grafana的数据源；

### 5.3 Etcd的写数据流程是什么？

- 总体上的请求流程从上至下依次为客户端 → API 接口层 → etcd Server → etcd raft 算法库。）
- 读请求：客户端通过负载均衡选择一个 etcd 节点发出读请求，API 接口层提供了 Range RPC 方法，etcd 服务端拦截到 gRPC 读请求后，调用相应的处理器处理请求。
- 写请求：客户端通过负载均衡选择一个 etcd 节点发起写请求，etcd 服务端拦截到 gRPC 写请求，涉及一些校验和监控，之后 KVServer 向 raft 模块发起提案，内容即为写入数据的命令。经过网络转发，当集群中的多数节点达成一致并持久化数据后，状态变更且 MVCC 模块执行提案内容。

### 5.4 kubevirt的基本组件及其作用？
- virt-api：kubevirt是以CRD形式去管理VM Pod，virt-api就是所有虚拟化操作的入口，这里面包括常规的CDR更新验证、以及console、vm start、stop等操作。

- virt-controller：virt-controller会根据vmi CRD，生成对应的virt-launcher Pod，并且维护CRD的状态。与kubernetes api-server通讯监控VMI资源的创建删除等状态。

- virt-handler：virt-handler会以deamonset形式部署在每一个节点上，负责监控节点上的每个虚拟机实例状态变化，一旦检测到状态的变化，会进行响应并且确保相应的操作能够达到所需（理想）的状态。virt-handler还会保持集群级别VMI Spec与相应libvirt域之间的同步；报告libvirt域状态和集群Spec的变化；调用以节点为中心的插件以满足VMI Spec定义的网络和存储要求。

- virt-launcher：每个virt-launcher，pod对应着一个VMI，kubelet只负责virt-launcher pod运行状态，不会去关心VMI创建情况。virt-handler会根据CRD参数配置去通知virt-launcher去使用本地的libvirtd实例来启动VMI，随着Pod的生命周期结束，virt-lanuncher也会去通知VMI去执行终止操作；其次在每个virt-launcher pod中还对应着一个libvirtd，virt-launcher通过libvirtd去管理VM的生命周期，这样做到去中心化，不再是以前的虚拟机那套做法，一个libvirtd去管理多个VM。

- virtctl：virtctl是kubevirt自带类似kubectl的命令行工具，它是越过virt-launcher pod这一层去直接管理VM虚拟机，可以控制VM的start、stop、restart。

### 5.5 k8s中的pod中OOM机制有了解过吗，原理是什么？

（tips：蚂蚁金服的oceanbase数据库产品问了这个问题，当时被问还是有点点蒙的。这个涉及到pod的QOS的相关知识。阿里面试官首先介绍了系统中进程的OOM机制，会给每个进程进行打分，然后kill掉得分最高的那一个进程，然后问pod的OOM是怎么样的？）

答：当系统 OOM上时，对于处理不同 OOMScore 的进程表现不同，OOMScore 是针对 memory 的，当宿主上 memory 不足时系统会优先 kill 掉 OOMScore 值高的进程，可以使用如下指令：
```bash
  $ cat /proc/$PID/oom_score
```
查看进程的 OOMScore。OOMScore 的取值范围为 [-1000, 1000]，Guaranteed pod 的默认值为 -998，Burstable pod 的值为 2~999，BestEffort pod 的值为 1000，也就是说当系统 OOM 时，首先会 kill 掉 BestEffort pod 的进程，若系统依然处于 OOM 状态，然后才会 kill 掉  Burstable pod，最后是 Guaranteed pod；












